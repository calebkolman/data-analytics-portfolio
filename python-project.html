<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-PKT8S4VJ');</script>
<!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Python Project</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKT8S4VJ"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
  <header>
    <h1>Content Moderation and User Engagement Insights for Schools</h1>
    <nav>
      <a href="index.html">Home</a>
    </nav>
  </header>
  <python-main>
  <div class="container">
    <python-section id="python-project">
      <h2>Objective</h2>
      <p>
        My goal was to identify trends and behaviors related to content moderation, including how flagged, deleted, ignored, or reported content affects user engagement. I also aimed to provide actionable insights to improve the efficiency of content review processes and optimize engagement across the platform.
      </p>
      <!-- Skip to Code Showcase Button -->
  <a href="#code-showcase" class="skip-button">Skip to Code Showcase</a>
</python-section>
    
      <h2>High-Level Findings</h2>
      <ul>
        <li><strong>Total Content:</strong> I analyzed over <strong>50,000 posts</strong> across all students.</li>
        <li><strong>Average Number of Posts Per Student:</strong> Each student averaged <strong>540 posts</strong>.</li>
        <li>
          <strong>Average Percent of Flagged Content:</strong> 
          <strong>5%</strong> of all content was flagged by my system for review. This suggests that students tend to have a manageable portion of content requiring moderation. They also seem to have less of a history and may moderate their own content more effectively than the average user.
        </li>
        <li>
          <strong>Average Percent of Deleted Content (Flagged Content):</strong> 
          Only <strong>4%</strong> of flagged content was deleted, indicating that a small proportion of flagged content is deemed inappropriate. This highlights that the system’s flagging mechanism may be overly sensitive, resulting in false positives.
        </li>
      </ul>

      <h3>Bar Graph 1</h3>
      <img src="notebooks/updated_days.png" alt="Bar Graph of Flagged Content Updates" style="max-width: 100%; height: auto; margin: 1rem 0;">
      <p>
        The chart above shows the timeframes in which students returned to update their flagged content after the initial scan. Most updates occurred on the same day the content was flagged, accounting for <strong>1,784 posts</strong>. A smaller number of updates happened after more than a week (<strong>730 posts</strong>), with minimal activity observed the next day (<strong>45 posts</strong>) or within a week (<strong>25 posts</strong>).
      </p>
      <p><strong>Key Insight:</strong> The majority of user updates happen immediately on the same day, while a smaller secondary group returns after an extended period of over a week.</p>

    <h3>Bar Graph 2</h3>
<img src="notebooks/python_thumbnail.png" alt="Bar Graph of Confidence Scores" style="max-width: 100%; height: auto; margin: 1rem 0;">
<p>
  The chart above illustrates the confidence scores of various content moderation categories, including <strong>toxicity, obscenity, insults, and polarizing content.</strong> 
  Observations indicate that confidence levels across most categories are relatively low:
</p>
<ul>
  <li><strong>Toxicity Confidence:</strong> The highest average score at <strong>0.45</strong>, indicating limited confidence in detecting toxic content.</li>
  <li><strong>Obscene Content Confidence:</strong> Slightly lower, averaging <strong>0.42</strong>.</li>
  <li><strong>Insults and Sexual Explicit Content:</strong> These categories show moderate scores of <strong>0.30</strong> and <strong>0.28</strong>, respectively.</li>
  <li><strong>Polarizing and Identity Attacks:</strong> The lowest scores, averaging <strong>0.10</strong>, suggest significant challenges in detecting these content types.</li>
</ul>
<p><strong>Key Insight:</strong> The low confidence scores across categories suggest that the content moderation model may struggle to classify inappropriate content accurately. This could explain why <strong>10%</strong> of flagged content is ignored and only <strong>4%</strong> is deleted. A more robust model with improved confidence levels could help reduce ignored flagged content and improve deletion rates.</p>

<h2>Conclusion</h2>
      <p>
        I analyzed how students interact with flagged content and the actions they take, such as deleting, ignoring, or reporting it. The majority of flagged content is addressed on the same day, while a smaller portion of students return after a week or more to address flagged content.
      </p>
      <p>
        My analysis revealed that <strong>10%</strong> of flagged content is ignored, compared to only <strong>4%</strong> being deleted. This suggests that much of the flagged content is not deemed inappropriate, or my flagging system might be overly sensitive (low confidence).
      </p>
<p>
  The analysis reveals that low confidence scores are a likely contributor to the system's inability to distinguish inappropriate content effectively. This could lead users to ignore flagged content rather than act on it. Improving the model’s confidence through enhanced datasets, better training processes, and targeted feature engineering will be critical to achieving better content moderation outcomes.
</p>
<p>
  By focusing on categories with the lowest confidence, such as polarizing content and identity attacks, we can ensure that the system more accurately flags inappropriate material, resulting in higher deletion rates and fewer ignored flags.
</p>


      <h2>Potential Next Steps</h2>
      <ul>
        <li>
          <strong>Review Flagging Sensitivity:</strong> 
          With <strong>10%</strong> of flagged content being ignored and only <strong>4%</strong> deleted, I plan to revisit the sensitivity of the flagging system to reduce false positives and ensure truly inappropriate content is flagged.
        </li>
        <li>
          <strong>User Education and Engagement:</strong> 
          The low percentage of reported content (<strong>0.04%</strong>) suggests that students may not fully understand the reporting feature. I propose introducing new engagement strategies, such as a <strong>"Train our AI Day"</strong>, to encourage students to interact with the system effectively while learning about the report feature. Additional incentives might help boost participation.
        </li>
        <li>
          <strong>Content Moderation Tools:</strong> 
          I aim to implement more intuitive tools for students to moderate their content, such as a swipe interface similar to apps like Tinder, to make moderation faster and more engaging.
        </li>
        <li>
          <strong>Deep Dive into Ignored Content:</strong> 
          I plan to investigate why a significant portion of flagged content is ignored by analyzing patterns or surveying users to understand their choices.
        </li>
        <li>
          <strong>Monitor Long-Term Engagement:</strong> 
          Most users only return to update content once. I propose sending email reminders about pending flagged content to encourage further engagement.
        </li>
      </ul>

    <python-section id="code-showcase">
 <h2>Python Code Showcase</h2>
<p>
  This project utilized Python extensively for exploratory data analysis, cleaning, statistical insights, and visualizations. Below are some key examples demonstrating my proficiency in working with data using Pandas, custom functions, and visualization tools like Matplotlib and Seaborn.
</p>

<h3>Exploratory Data Analysis</h3>
<pre><code>
# Check data types
print(data.dtypes)

# Check the shape (number of rows and columns)
print(data.shape)

# Get summary statistics
print(data.describe(include='all'))

# Group by 'business_customer_id' to count the number of posts for each user
posts_per_student = data.groupby('business_customer_id')['id'].count()
</code></pre>

<h3>Data Cleaning and Transformation</h3>
<pre><code>
# Drop rows with any missing values
data_cleaned = data.dropna()

# Convert 'createdAt' and 'updatedAt' to datetime
data_cleaned['createdAt'] = pd.to_datetime(data_cleaned['createdAt'], errors='coerce')
data_cleaned['updatedAt'] = pd.to_datetime(data_cleaned['updatedAt'], errors='coerce')

# Calculate the difference in days between 'createdAt' and 'updatedAt'
data_cleaned['update_duration_days'] = (data_cleaned['updatedAt'] - data_cleaned['createdAt']).dt.days
</code></pre>

<h3>Custom Function for Classification</h3>
<pre><code>
# Define a function to classify the date difference
def classify_update_duration(days):
    if days == 0:
        return 'Same Day'
    elif days == 1:
        return 'Next Day'
    elif days <= 7:
        return 'Within a Week'
    else:
        return 'More than a Week'
</code></pre>

<h3>Statistical Insights and Visualization</h3>
<pre><code>
# Convert the counts to a DataFrame
classification_counts_df = classification_counts.reset_index()
classification_counts_df.columns = ['Update Duration', 'Count']

# Sort the values by the custom category order
classification_counts = classification_counts.sort_index()

# Set style
sns.set(style="whitegrid")

# Create a figure and axis object
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the bar chart
classification_counts.plot(kind='bar', color=sns.color_palette("Blues_r", len(classification_counts)), ax=ax)

# Set title
plt.title('Days Returned After Initial Scan', fontsize=16, weight='bold')

# Set x and y labels
plt.xlabel('Timeframe Returned', fontsize=14, weight='bold')
plt.ylabel('Number of Posts Updated', fontsize=14, weight='bold')

# Rotate x-axis labels
plt.xticks(rotation=45, fontsize=12)

# Add gridlines
plt.grid(True, which='both', axis='y', linestyle='--', linewidth=0.7)

# Add data labels
for idx, value in enumerate(classification_counts):
    ax.text(idx, value + 1, str(value), ha='center', va='bottom', fontsize=12)

# Display plot
plt.tight_layout()
plt.show()
</code></pre>

<p>
  These examples highlight my ability to clean and transform data, derive statistical insights, and create professional-quality visualizations that communicate actionable findings effectively.
</p>



      <h3>Explore the Full Jupyter Notebook</h3>
      <ul>
        <li><a href="https://github.com/calebkolman/data-analytics-portfolio/blob/python-main/notebooks/School_Content_Analysis.ipynb" target="_blank">View on GitHub</a></li>
      </ul>
    </python-section>
  </python-main>
  <footer>
    <p>&copy; 2024 Caleb Kolman. All rights reserved.</p>
  </footer>
</body>
</html>
